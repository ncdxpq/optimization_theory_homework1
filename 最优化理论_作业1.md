# 						  最优化理论__作业1

**学号：406100210008**  					**专业：计算机技术**

**姓名：潘奇**   									  **手机号：18636940978**

## 文档结构:

1.作业题目	2.无约束优化问题相关以及题目设定	3.梯度下降法解决无约束优化问题

4.牛顿法解决无约束优化问题	5.BFGS方法解决无约束优化问题	 6.作业总结

## 1.作业题目

编程实现梯度下降法，牛顿法，BFGS方法，并自选一个无约束优化问题，分别调用三个方法，形成demo

**要求**：体现三个代码的核心代码解释，优化问题简介，demo演示，结果分析等;禁用第三方API,禁止抄袭

## 2.无约束优化问题相关以及题目设定

无约束最优化问题：即不对定义域或值域做任何限制的情况下，求解函数f(x)的最小值。

以下代码均为求解函数![clip_image001](C:\Users\枫\AppData\Local\Temp\msohtmlclip1\01\clip_image001.png)的最小值，易知该问题满足无约束优化问题的条件，且最优解为x=y=1,只需查看求出的解与最优解的差距即可。

## 3.**梯度下降法**解决无约束优化问题

#### 3.1梯度下降法介绍：

梯度：函数值上升的最快的方向。

梯度下降法的目标是：找到一组解可以最小化函数值。

基本思想：采用负梯度方向作为搜索方向![mathpix 2021-10-24 15-44-57_副本](C:\Users\枫\Desktop\mathpix 2021-10-24 15-44-57_副本.png)。

原理：因为负梯度方向是当前点处使函数值下降最快的方向。

#### 3.2代码实现

没有引入任何第三方API，仅仅使用了random.random函数

```python
from random import random
```

首先定义函数，以及对应的dx，dy

```python
def function(x, y):  # 二元函数f(x,y)。显然x=1,y=1时，函数可以取最小值
    return 100 * ((x ** 2 - y) ** 2) + (x - 1) ** 2
```

```python
def dx(x, y):  # 对x求偏导
    return 400*x*(x**2-y) + 2*(x-1)
```

```python
def dy(x, y):  # 对y求偏导
    return -200*(x**2-y)
```

之后定义了梯度下降法的主函数：

```python
def gradient_decent(function, function_gradient, n_variables,
                    lr, max_iterations, epsilon):
    """给一个函数y = f(theta),用 梯度下降法使y最小化
    Args:
        function: 要优化的函数.
        function_gradient: y对于各个变量的偏导数
        n_variables (int): 变量的个数
        lr (float, optional): 学习率，默认:0.1
        max_iterations (int, optional): 最大迭代次数，默认是10000
        epsilon (float, optional): 默认1e-5，希望结果更加精确，可以把tolerance调小一些。
    Returns:
        tuple: Theta, y.
    """

    # 1.随机生成对应变量个数的theta，eg:[0.5109615984946895, 0.039538100076638716]
    theta = [random() for _ in range(n_variables)]  # random.random():生成一个随机的浮点数，在0~1之间
    # 第一步theta代入各个变量中求得的第一步的值
    f_theta = function(*theta)
    # 开始迭代，最高迭代max_iter次
    for times in range(max_iterations):
        print("第{}次迭代,此时对应的函数值是{}".format(times, f_theta))

        # 2.计算当前每个theta的梯度(偏导数)，eg: [0.009278799682036265, -0.005734613578301406]
        gradient = [f(*theta) for f in function_gradient]

        # 3.通过梯度更新每个theta
        for element in range(n_variables):
            theta[element] -= gradient[element] * lr

        # 4.如果函数收敛或者超过最大迭代次数则返回
        f_theta, y_pre = function(*theta), f_theta
        if abs(y_pre - f_theta) < epsilon:
            break
    return theta, f_theta

```

#### 3.3结果分析

最后运行main()主函数，获得结果

```python
# 求解函数最小值
def main():
    print("--------开始解题--------:")
    theta, f_theta = gradient_decent(function,
                                     [dx, dy],
                                     n_variables=2,
                                     lr=0.001,
                                     max_iterations=5000000,
                                     epsilon=1e-5)
    print("最终的结果是:\n各个变量值:{}, 对应的最终函数的值:{}\n".format(theta, f_theta))
```

运行一下，获得的结果部分如下所示：迭代次数很多，学习率设置的比较低

-----------------------------------------------------------------------------------------------------------------------------------------------------------

第2920次迭代,此时对应的函数值是0.01066288751946853
第2921次迭代,此时对应的函数值是0.010652794753025363
第2922次迭代,此时对应的函数值是0.010642712375978176
第2923次迭代,此时对应的函数值是0.010632640376352311
第2924次迭代,此时对应的函数值是0.010622578742189007
第2925次迭代,此时对应的函数值是0.010612527461545312
第2926次迭代,此时对应的函数值是0.010602486522494105
第2927次迭代,此时对应的函数值是0.010592455913124023
第2928次迭代,此时对应的函数值是0.010582435621539509
第2929次迭代,此时对应的函数值是0.010572425635860703
最终的结果是:
各个变量值:[0.8973192252403076, 0.8047449345838134], 对应的最终函数的值:0.010562425944223495

-----------------------------------------------------------------------------------------------------------------------------------------------------------

可以看出以及很接近最小值0了，易知最优解为x=y=1，这里求得的解x=0.89,y=0.80,可以看出梯度下降法产生了作用！

## 4.牛顿法解决无约束优化问题

#### 4.1牛顿法介绍：

基本思想：用![mathpix 2021-10-24 15-47-25_副本](C:\Users\枫\Desktop\mathpix 2021-10-24 15-47-25_副本.png)处的二阶泰勒展开式近似原目标函数，然后求该二阶展开式的最小值

#### 4.2代码实现：

没有引入任何第三方API仅仅使用了random和numpy函数库

```python
from random import random
import numpy as np
```

然后仍然是题目设定：

```python
# 函数表达式function
function = lambda x: 100 * (x[0] ** 2 - x[1]) ** 2 + (x[0] - 1) ** 2

# 梯度向量 gradient
gradient = lambda x: np.array([400 * x[0] * (x[0] ** 2 - x[1]) + 2 * (x[0] - 1), -200 * (x[0] ** 2 - x[1])])

# 海森矩阵 hessian_matrix:二阶偏导数，[[f''xx,f''xy],[f''yx,f''yy]]
hessian_matrix = lambda x: np.array([[1200 * x[0] ** 2 - 400 * x[1] + 2, -400 * x[0]], [-400 * x[0], 200]])
```

最后定义牛顿法：

```python
# 用牛顿法求解无约束优化问题
def newton_method(function, gradient, hessian_matrix, epsilon, max_iterations):
    """给一个函数y = f(theta),用牛顿法使y最小化
        Args:
            function: 要优化的函数.
            gradient: y的第一个偏导数
            hessian_matrix:海森矩阵（二阶偏导数矩阵）
            epsilon (float, optional): 默认1e-5，希望结果更加精确，可以把epsilon调小一些
            max_iterations (int, optional): 最大迭代次数，默认是10000
        Returns:
            tuple: Theta, function_theta,iterations
        """
    theta = [random() for _ in range(2)]  # 随机初始化
    iterations = 0

    while iterations < max_iterations:
        gradient_k = gradient(theta)
        hessian_k = hessian_matrix(theta)
        
        # 以矩阵形式解一个线性矩阵方程，或线性标量方程组
        dk = -1.0 * np.linalg.solve(hessian_k, gradient_k)
        iterations += 1
        print('第{}次迭代，此时对应的函数值是{}'.format(iterations, function(theta)))
        if np.linalg.norm(dk) < epsilon:
            break
        theta += dk
    return theta, function(theta), iterations
```

#### 4.3结果分析

运行主函数，获得该函数的最优解

```python
if __name__ == "__main__":
    theat, f_theta, iteration = newton_method(function,
                                              gradient,
                                              hessian_matrix,
                                              epsilon=1e-5,
                                              max_iterations=500)

    print('--------开始解题--------')
    print('最终求出的各个theta为{}'.format(theat))
    print('最终的函数值为是{}'.format(f_theta))
    print('迭代次数为:{}'.format(iteration))
```

输出结果如下所示：

-----------------------------------------------------------------------------------------------------------------------------------------------------------

第1次迭代，此时对应的函数值是0.030766621219147902
第2次迭代，此时对应的函数值是0.09234659186723068
第3次迭代，此时对应的函数值是0.04096275875299265
第4次迭代，此时对应的函数值是0.021507701571380294
第5次迭代，此时对应的函数值是0.004046488284135643
第6次迭代，此时对应的函数值是0.0009784778189334958
第7次迭代，此时对应的函数值是9.676749326454374e-06
第8次迭代，此时对应的函数值是9.302119223959663e-09
第9次迭代，此时对应的函数值是9.595425044875697e-16
--------开始解题--------
最终求出的各个theta为[0.99999997 0.99999994]
最终的函数值为是9.595425044875697e-16
迭代次数为:9

-----------------------------------------------------------------------------------------------------------------------------------------------------------

可以看出，牛顿法的迭代次数很少，得出的解也更加精确，与真正的解x=1,y=1非常接近。

## 5.BFGS方法解决无约束优化问题

#### 5.1 BFGS算法简介：

BFGS 算法是使用较多的一种拟牛顿方法，是由 Broyden ， Fletcher ， Goldfarb ， Shanno 四个人分别提出的，故称为 BFGS 校正。



#### 5.2代码实现：

代码没有引用任何第三方API，仅仅使用了numpy这一个库

```python
import numpy as np
```

首先设定和以上两种方法一样的函数与梯度计算公式：

```python
# function
def function(x):
    return 100 * (x[0, 0] ** 2 - x[1, 0]) ** 2 + (x[0, 0] - 1) ** 2


# gradient_function
def gradient_function(x):
    """获得对x和 y的偏导数
    """
    result = np.zeros((2, 1))
    # 对x求偏导数
    result[0, 0] = 400 * x[0, 0] * (x[0, 0] ** 2 - x[1, 0]) + 2 * (x[0, 0] - 1)
    # 对y求偏导数
    result[1, 0] = -200 * (x[0, 0] ** 2 - x[1, 0])
    return result
```

定义BFGS算法：

```python
def BFGS(function, gradient_function, theta, epsilon, max_iterations):
    rho = 0.55
    sigma = 0.4
    m = np.shape(theta)[0]  # 取出第一个变量
    Bk = np.eye(m)  # m阶单位矩阵
    times = 0

    while (times < max_iterations):  # 规定最大迭代次数
        gk = np.mat(gradient_function(theta))  # 计算梯度(求得此时对于两个变量的偏导数)
        dk = np.mat(-np.linalg.solve(Bk, gk))
        mk = 0

        for i in range(20):
            new_function = function(theta + rho ** i * dk)
            old_function = function(theta)
            if (new_function < old_function + sigma * (rho ** i) * (gk.T * dk)[0, 0]):
                mk = i
                break


        # BFGS校正
        y_pre = function(theta)
        theta_temp = theta + rho ** mk * dk
        sk = theta_temp - theta
        yk = gradient_function(theta_temp) - gk
        if (yk.T * sk > 0):
            Bk = Bk - (Bk * sk * sk.T * Bk) / (sk.T * Bk * sk) + (yk * yk.T) / (yk.T * sk)
        times = times + 1
        print('第{}次迭代\n'.format(times))
        theta = theta_temp
        print('当前各个变量的值为:{}'.format(theta_temp))
        f_theta = function(theta)
        print('当前函数值为：{}'.format(f_theta))
        if abs(y_pre - f_theta) < epsilon:
            break
    return f_theta, theta
```

#### 5.3结果分析：

运行以下代码获得结果

```python
if __name__ == "__main__":

    theta = np.random.rand(2, 1)
    f_theta, theta = BFGS(function,
                          gradient_function,
                          theta,
                          epsilon=1e-5,
                          max_iterations=500)

    print('最终的各个变量值为:{}'.format(theta))
    print('最终的函数值为：{}'.format(f_theta))
```

输出结果展示：

-----------------------------------------------------------------------------------------------------------------------------------------------------------

......
第12次迭代

当前各个变量的值为:[[0.98963417]
 [0.9781957 ]]
当前函数值为：0.0002467132113611151
第13次迭代

当前各个变量的值为:[[0.99702842]
 [0.99388971]]
当前函数值为：1.1926404122910604e-05
第14次迭代

当前各个变量的值为:[[0.99969589]
 [0.99935121]]
当前函数值为：2.5782773300461385e-07
第15次迭代

当前各个变量的值为:[[0.99995605]
 [0.99991675]]
当前函数值为：4.0980822934398004e-09
最终的各个变量值为:[[0.99995605]
 [0.99991675]]
最终的函数值为：4.0980822934398004e-09

-----------------------------------------------------------------------------------------------------------------------------------------------------------

可以看出得出结果的精度也非常高，迭代次数也很少。

## 6.总结

以上代码均以保存到我本人的github中，作为学习过程中珍贵的记录，把链接奉上：

https://github.com/ncdxpq/optimization_theory_homework1

感谢肖艳阳老师的课程，在最优化理论这门课中，我学习到了非常多的数学知识，是肖老师让我进一步理解了数学的美妙之处。通过完成本次作业，我对梯度下降法等具有了更深的理解，对神经网络工作原理有了更加深刻的认识，谢谢！

